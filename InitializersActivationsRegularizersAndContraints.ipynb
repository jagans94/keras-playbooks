{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of initializers\n",
    "\n",
    "Initializations define the way to set the initial random weights of Keras layers.\n",
    "\n",
    "The keyword arguments used for passing initializers to layers will depend on the layer. Usually it is simply kernel_initializer and bias_initializer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "Dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer = Dense(10, kernel_initializer='lecun_uniform', bias_initializer='ones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "layer = Dense(10, kernel_initializer='he_normal', bias_initializer=Constant(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are plenty of initializers, you can even make your own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x113292750>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def my_init(shape, dtype=None):\n",
    "    return K.random_normal(shape, dtype=dtype)\n",
    "\n",
    "Dense(64, kernel_initializer=my_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of activations\n",
    "\n",
    "Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense, Input\n",
    "\n",
    "x = Input((1,))\n",
    "x = Dense(64)(x)\n",
    "x = Activation('tanh')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input((1,))\n",
    "x = Dense(64, activation='tanh')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass an element-wise Tensorflow/Theano function as an activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "x = Input((1,))\n",
    "x = Dense(64, activation=K.tanh)(x)\n",
    "x = Activation(K.tanh)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of regularizers\n",
    "\n",
    "Regularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes.\n",
    "\n",
    "The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers Dense, Conv1D, Conv2D and Conv3D have a unified API.\n",
    "\n",
    "These layers expose 3 keyword arguments:\n",
    "\n",
    "* kernel_regularizer: instance of keras.regularizers.Regularizer\n",
    "* bias_regularizer: instance of keras.regularizers.Regularizer\n",
    "* activity_regularizer: instance of keras.regularizers.Regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x113326090>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.regularizers.L1L2 at 0x113292350>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# available regularizers\n",
    "regularizers.l1(0.)\n",
    "regularizers.l2(0.)\n",
    "regularizers.l1_l2(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x113351cd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom regularizer\n",
    "from keras import backend as K\n",
    "\n",
    "def l1_reg(weight_matrix):\n",
    "    return 0.01 * K.sum(K.abs(weight_matrix))\n",
    "\n",
    "Dense(64, input_dim=64,\n",
    "                kernel_regularizer=l1_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition there is the activity regularization layer that can help with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import ActivityRegularization\n",
    "\n",
    "ActivityRegularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of constraints\n",
    "\n",
    "Functions from the constraints module allow setting constraints (eg. non-negativity) on network parameters during optimization.\n",
    "\n",
    "The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers Dense, Conv1D, Conv2D and Conv3D have a unified API.\n",
    "\n",
    "These layers expose 2 keyword arguments:\n",
    "\n",
    "* kernel_constraint for the main weights matrix\n",
    "* bias_constraint for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x113292990>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.constraints import max_norm\n",
    "\n",
    "Dense(64, kernel_constraint=max_norm(2.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available constraints\n",
    "\n",
    "* max_norm(max_value=2, axis=0): maximum-norm constraint\n",
    "* non_neg(): non-negativity constraint\n",
    "* unit_norm(): unit-norm constraint, enforces the matrix to have unit norm along the last axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "So you can apply all the concepts to a core layer or use them to make your own! Below I show you how to make a layer that uses all of the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras.activations import hard_sigmoid\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      contraint='unit_norm',\n",
    "                                      regularizer=regularizers.l1(1.),\n",
    "                                      trainable=True)\n",
    "        \n",
    "        # Another way to enable this regularization is with the add loss function\n",
    "        # self.add_loss(self.kernel, inputs=None)\n",
    "        \n",
    "        super(MyLayer, self).build(input_shape) \n",
    "\n",
    "    def call(self, x):\n",
    "        return hard_sigmoid(K.dot(x, self.kernel))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this all applied to the add_weight function. Both the initializer and the constraint can only be applied there! Regularization as you can see, can be applied up and down the pipe. And activations are pretty self evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
