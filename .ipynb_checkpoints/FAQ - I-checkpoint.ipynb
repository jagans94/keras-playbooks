{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I run a Keras model on multiple GPUs? \n",
    "\n",
    "We recommend doing so using the **TensorFlow** backend. There are two ways to run a single model on multiple GPUs: **data parallelism** and **device parallelism**.\n",
    "\n",
    "In most cases, what you need is most likely data parallelism.\n",
    "\n",
    "#### Data parallelism\n",
    "\n",
    "Data parallelism consists in replicating the target model once on each device, and using each replica to process a different fraction of the input data.\n",
    "Keras has a built-in utility, `keras.utils.multi_gpu_model`, which can produce a data-parallel version of any model, and achieves quasi-linear speedup on up to 8 GPUs.\n",
    "\n",
    "For more information, see the documentation for [multi_gpu_model](/utils/#multi_gpu_model). Here is a quick example:\n",
    "\n",
    "```python\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "# Replicates `model` on 8 GPUs.\n",
    "# This assumes that your machine has 8 available GPUs.\n",
    "parallel_model = multi_gpu_model(model, gpus=8)\n",
    "parallel_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer='rmsprop')\n",
    "\n",
    "# This `fit` call will be distributed on 8 GPUs.\n",
    "# Since the batch size is 256, each GPU will process 32 samples.\n",
    "parallel_model.fit(x, y, epochs=20, batch_size=256)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device parallelism\n",
    "\n",
    "Device parallelism consists in running different parts of a same model on different devices. It works best for models that have a parallel architecture, e.g. a model with two branches.\n",
    "\n",
    "This can be achieved by using TensorFlow device scopes. Here is a quick example:\n",
    "\n",
    "```python\n",
    "# Model where a shared LSTM is used to encode two different sequences in parallel\n",
    "input_a = keras.Input(shape=(140, 256))\n",
    "input_b = keras.Input(shape=(140, 256))\n",
    "\n",
    "shared_lstm = keras.layers.LSTM(64)\n",
    "\n",
    "# Process the first sequence on one GPU\n",
    "with tf.device_scope('/gpu:0'):\n",
    "    encoded_a = shared_lstm(tweet_a)\n",
    "# Process the next sequence on another GPU\n",
    "with tf.device_scope('/gpu:1'):\n",
    "    encoded_b = shared_lstm(tweet_b)\n",
    "\n",
    "# Concatenate results on CPU\n",
    "with tf.device_scope('/cpu:0'):\n",
    "    merged_vector = keras.layers.concatenate([encoded_a, encoded_b],\n",
    "                                             axis=-1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I save a Keras model?\n",
    "\n",
    "#### Saving/loading whole models (architecture + weights + optimizer state)\n",
    "\n",
    "*It is not recommended to use pickle or cPickle to save a Keras model.*\n",
    "\n",
    "You can use `model.save(filepath)` to save a Keras model into a single HDF5 file which will contain:\n",
    "\n",
    "- the architecture of the model, allowing to re-create the model\n",
    "- the weights of the model\n",
    "- the training configuration (loss, optimizer)\n",
    "- the state of the optimizer, allowing to resume training exactly where you left off.\n",
    "\n",
    "You can then use `keras.models.load_model(filepath)` to reinstantiate your model.\n",
    "`load_model` will also take care of compiling the model using the saved training configuration\n",
    "(unless the model was never compiled in the first place).\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jagan\\Anaconda3\\envs\\image-classification\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 400))\n",
    "labels = np.random.randint(2, size=(1000, 10))\n",
    "\n",
    "# Creating 4-layered Model\n",
    "inputs = Input(shape=(400,))\n",
    "x = Dense(50, activation='relu')(inputs)\n",
    "x = Dense(25, activation='relu')(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                20050     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                260       \n",
      "=================================================================\n",
      "Total params: 21,585\n",
      "Trainable params: 21,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jagan\\Anaconda3\\envs\\image-classification\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2857: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Jagan\\Anaconda3\\envs\\image-classification\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 0s 290us/step - loss: 11.5873 - acc: 0.0530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2559b7759b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                20050     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                260       \n",
      "=================================================================\n",
      "Total params: 21,585\n",
      "Trainable params: 21,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving/loading only a model's architecture\n",
    "\n",
    "If you only need to save the **architecture of a model**, and not its weights or its training configuration, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_json?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as JSON\n",
    "json_string = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_yaml?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as YAML\n",
    "yaml_string = model.to_yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yaml_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated JSON / YAML files are human-readable and can be manually edited if needed.\n",
    "\n",
    "You can then build a fresh model from this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model reconstruction from JSON:\n",
    "from keras.models import model_from_json\n",
    "model = model_from_json(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_json?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model reconstruction from YAML\n",
    "from keras.models import model_from_yaml\n",
    "model = model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_yaml?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving/loading only a model's weights\n",
    "\n",
    "If you need to save the **weights of a model**, you can do so in HDF5 with the code below.\n",
    "\n",
    "Note that you will first need to install HDF5 and the Python library h5py, which do not come bundled with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have code for instantiating your model, you can then load the weights you saved into a model with the *same* architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to load weights into a *different* architecture (with some layers in common), for instance for fine-tuning or transfer-learning, you can load weights by *layer name*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('my_model_weights.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.7469 - acc: 0.4700\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "data = np.random.random((100,3) )\n",
    "labels = np.random.randint(2,size=(100,1))\n",
    "\n",
    "# Original Model\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=3, name='dense_1'))\n",
    "model.add(Dense(1, name='dense_2', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(data,labels,epochs=1)\n",
    "\n",
    "model.save_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.46778956, -0.83169574, -0.23499848,  0.6202264 ,  0.52816695],\n",
       "        [-0.8403742 ,  0.59159976, -0.40805045, -0.5142242 , -0.24679546],\n",
       "        [-0.44778636, -0.27247843, -0.51811975,  0.86169785, -0.71557754]],\n",
       "       dtype=float32),\n",
       " array([ 0.00923148,  0.00923403,  0.00923689,  0.00926259, -0.0092677 ],\n",
       "       dtype=float32),\n",
       " array([[ 0.5259221 ],\n",
       "        [ 0.6754761 ],\n",
       "        [ 0.76037115],\n",
       "        [ 0.8585852 ],\n",
       "        [-0.54912275]], dtype=float32),\n",
       " array([0.00924978], dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.46778956, -0.83169574, -0.23499848,  0.6202264 ,  0.52816695],\n",
      "       [-0.8403742 ,  0.59159976, -0.40805045, -0.5142242 , -0.24679546],\n",
      "       [-0.44778636, -0.27247843, -0.51811975,  0.86169785, -0.71557754]],\n",
      "      dtype=float32), array([ 0.00923148,  0.00923403,  0.00923689,  0.00926259, -0.0092677 ],\n",
      "      dtype=float32), array([[-0.4582802 , -0.48293453, -0.5134464 , -0.6202377 ,  0.36491305,\n",
      "         0.3125919 ,  0.45678574,  0.04675943,  0.59464604, -0.37079608],\n",
      "       [ 0.4533816 ,  0.3507092 , -0.01868623,  0.56698924, -0.44100505,\n",
      "         0.51859444,  0.01338631,  0.13807279, -0.57183224, -0.37812504],\n",
      "       [ 0.39718097, -0.5171029 ,  0.12943196,  0.00592238,  0.38930136,\n",
      "         0.45920557, -0.62662935, -0.6158292 ,  0.5728635 ,  0.5977387 ],\n",
      "       [ 0.17626274, -0.4314391 , -0.5476793 ,  0.04908943,  0.17118835,\n",
      "         0.4347853 ,  0.47455424, -0.03049487, -0.50782514,  0.52301854],\n",
      "       [-0.5988344 , -0.01834518, -0.38523114, -0.58332795,  0.45345205,\n",
      "         0.6263403 ,  0.2723878 ,  0.45925277,  0.13736647,  0.4171998 ]],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# New model\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=3, name='dense_1'))  # will be loaded\n",
    "model.add(Dense(10, name='new_dense'))  # will not be loaded\n",
    "\n",
    "# load weights from first model; will only affect the first layer, dense_1.\n",
    "model.load_weights('my_model_weights.h5', by_name=True)\n",
    "\n",
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling custom layers (or other custom objects) in saved models ~ *Pending Implementation*\n",
    "\n",
    "If the model you want to load includes custom layers or other custom classes or functions, \n",
    "you can pass them to the loading mechanism via the `custom_objects` argument: \n",
    "\n",
    "```python\n",
    "from keras.models import load_model\n",
    "# Assuming your model includes instance of an \"AttentionLayer\" class\n",
    "model = load_model('my_model.h5', custom_objects={'AttentionLayer': AttentionLayer})\n",
    "```\n",
    "\n",
    "Alternatively, you can use a [custom object scope](https://keras.io/utils/#customobjectscope):\n",
    "\n",
    "```python\n",
    "from keras.utils import CustomObjectScope\n",
    "\n",
    "with CustomObjectScope({'AttentionLayer': AttentionLayer}):\n",
    "    model = load_model('my_model.h5')\n",
    "```\n",
    "\n",
    "Custom objects handling works the same way for `load_model`, `model_from_json`, `model_from_yaml`:\n",
    "\n",
    "```python\n",
    "from keras.models import model_from_json\n",
    "model = model_from_json(json_string, custom_objects={'AttentionLayer': AttentionLayer})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I obtain the output of an intermediate layer?\n",
    "\n",
    "One simple way is to create a new `Model` that will output the layers that you are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 0s 348us/step - loss: 11.3414 - acc: 0.2460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2559da47128>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "# Generating Data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 400))\n",
    "labels = np.random.randint(2, size=(1000, 10))\n",
    "\n",
    "# Creating 4-layered Model\n",
    "inputs = Input(shape=(400,))\n",
    "x = Dense(50, activation='relu')(inputs)\n",
    "x = Dense(25, activation='relu', name='my_layer')(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                20050     \n",
      "_________________________________________________________________\n",
      "my_layer (Dense)             (None, 25)                1275      \n",
      "=================================================================\n",
      "Total params: 21,325\n",
      "Trainable params: 21,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer_name = 'my_layer'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(data)\n",
    "intermediate_layer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can build a Keras function that will return the output of a certain layer given a certain input, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a Sequential model\n",
    "get_3rd_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[3].output])\n",
    "layer_output = get_3rd_layer_output([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you could build a Theano and TensorFlow function directly.\n",
    "\n",
    "Note that if your model has a different behavior in training and testing phase (e.g. if it uses `Dropout`, `BatchNormalization`, etc.), you will need\n",
    "to pass the learning phase flag to your function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                  [model.layers[3].output])\n",
    "\n",
    "# output in test mode = 0\n",
    "layer_output = get_3rd_layer_output([data, 0])[0]\n",
    "\n",
    "# output in train mode = 1\n",
    "layer_output = get_3rd_layer_output([data, 1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I use Keras with datasets that don't fit in memory?\n",
    "\n",
    "You can do batch training using `model.train_on_batch(x, y)` and `model.test_on_batch(x, y)`. See the [models documentation](/models/sequential).\n",
    "\n",
    "Alternatively, you can write a generator that yields batches of training data and use the method `model.fit_generator(data_generator, steps_per_epoch, epochs)`.\n",
    "\n",
    "You can see batch training in action in our [CIFAR10 example](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I interrupt training when the validation loss isn't decreasing anymore?\n",
    "\n",
    "You can use an `EarlyStopping` callback:\n",
    "\n",
    "```python\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model.fit(x, y, validation_split=0.2, callbacks=[early_stopping])\n",
    "```\n",
    "\n",
    "Find out more in the [callbacks documentation](/callbacks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is the validation split computed?\n",
    "\n",
    "If you set the `validation_split` argument in `model.fit` to e.g. 0.1, then the validation data used will be the *last 10%* of the data. If you set it to 0.25, it will be the last 25% of the data, etc. Note that the data isn't shuffled before extracting the validation split, so the validation is literally just the *last* x% of samples in the input you passed.\n",
    "\n",
    "The same validation set is used for all epochs (within a same call to `fit`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the data shuffled during training?\n",
    "\n",
    "Yes, if the `shuffle` argument in `model.fit` is set to `True` (which is the default), the training data will be randomly shuffled at each epoch.\n",
    "\n",
    "Validation data is never shuffled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I record the training / validation loss / accuracy at each epoch?\n",
    "\n",
    "The `model.fit` method returns an `History` callback, which has a `history` attribute containing the lists of successive losses and other metrics.\n",
    "\n",
    "```python\n",
    "hist = model.fit(x, y, validation_split=0.2)\n",
    "print(hist.history)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I \"freeze\" Keras layers?\n",
    "\n",
    "To \"freeze\" a layer means to exclude it from training, i.e. its weights will never be updated. This is useful in the context of fine-tuning a model, or using fixed embeddings for a text input.\n",
    "\n",
    "You can pass a `trainable` argument (boolean) to a layer constructor to set a layer to be non-trainable:\n",
    "\n",
    "```python\n",
    "frozen_layer = Dense(32, trainable=False)\n",
    "```\n",
    "Additionally, you can set the `trainable` property of a layer to `True` or `False` after instantiation. For this to take effect, you will need to call `compile()` on your model after modifying the `trainable` property. Here's an example:\n",
    "\n",
    "```python\n",
    "x = Input(shape=(32,))\n",
    "layer = Dense(32)\n",
    "layer.trainable = False\n",
    "y = layer(x)\n",
    "\n",
    "frozen_model = Model(x, y)\n",
    "# in the model below, the weights of `layer` will not be updated during training\n",
    "frozen_model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "layer.trainable = True\n",
    "trainable_model = Model(x, y)\n",
    "# with this model the weights of the layer will be updated during training\n",
    "# (which will also affect the above model since it uses the same layer instance)\n",
    "trainable_model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "frozen_model.fit(data, labels)  # this does NOT update the weights of `layer`\n",
    "trainable_model.fit(data, labels)  # this updates the weights of `layer`\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I use stateful RNNs?\n",
    "\n",
    "Making a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch.\n",
    "\n",
    "When using stateful RNNs, it is therefore assumed that:\n",
    "\n",
    "- all batches have the same number of samples\n",
    "- If `x1` and `x2` are successive batches of samples, then `x2[i]` is the follow-up sequence to `x1[i]`, for every `i`.\n",
    "\n",
    "To use statefulness in RNNs, you need to:\n",
    "\n",
    "- explicitly specify the batch size you are using, by passing a `batch_size` argument to the first layer in your model. E.g. `batch_size=32` for a 32-samples batch of sequences of 10 timesteps with 16 features per timestep.\n",
    "- set `stateful=True` in your RNN layer(s).\n",
    "- specify `shuffle=False` when calling fit().\n",
    "\n",
    "To reset the states accumulated:\n",
    "\n",
    "- use `model.reset_states()` to reset the states of all layers in the model\n",
    "- use `layer.reset_states()` to reset the states of a specific stateful RNN layer\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "\n",
    "x  # this is our input data, of shape (32, 21, 16)\n",
    "# we will feed it to our model in sequences of length 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(10, 16), batch_size=32, stateful=True))\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# we train the network to predict the 11th timestep given the first 10:\n",
    "model.train_on_batch(x[:, :10, :], np.reshape(x[:, 10, :], (32, 16)))\n",
    "\n",
    "# the state of the network has changed. We can feed the follow-up sequences:\n",
    "model.train_on_batch(x[:, 10:20, :], np.reshape(x[:, 20, :], (32, 16)))\n",
    "\n",
    "# let's reset the states of the LSTM layer:\n",
    "model.reset_states()\n",
    "\n",
    "# another way to do it in this case:\n",
    "model.layers[0].reset_states()\n",
    "```\n",
    "\n",
    "Notes that the methods `predict`, `fit`, `train_on_batch`, `predict_classes`, etc. will *all* update the states of the stateful layers in a model. This allows you to do not only stateful training, but also stateful prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I remove a layer from a Sequential model?\n",
    "\n",
    "You can remove the last added layer in a Sequential model by calling `.pop()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=784))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "print(len(model.layers))  # \"2\"\n",
    "\n",
    "model.pop()\n",
    "print(len(model.layers))  # \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I use pre-trained models in Keras?\n",
    "\n",
    "Code and pre-trained weights are available for the following image classification models:\n",
    "\n",
    "- Xception\n",
    "- VGG16\n",
    "- VGG19\n",
    "- ResNet50\n",
    "- Inception v3\n",
    "- Inception-ResNet v2\n",
    "- MobileNet v1\n",
    "\n",
    "They can be imported from the module `keras.applications`:\n",
    "\n",
    "```python\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "```\n",
    "\n",
    "For a few simple usage examples, see [the documentation for the Applications module](/applications).\n",
    "\n",
    "For a detailed example of how to use such a pre-trained model for feature extraction or for fine-tuning, see [this blog post](http://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html).\n",
    "\n",
    "The VGG16 model is also the basis for several Keras example scripts:\n",
    "\n",
    "- [Style transfer](https://github.com/keras-team/keras/blob/master/examples/neural_style_transfer.py)\n",
    "- [Feature visualization](https://github.com/keras-team/keras/blob/master/examples/conv_filter_visualization.py)\n",
    "- [Deep dream](https://github.com/keras-team/keras/blob/master/examples/deep_dream.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I use HDF5 inputs with Keras?\n",
    "\n",
    "You can use the `HDF5Matrix` class from `keras.utils.io_utils`. See [the HDF5Matrix documentation](/utils/#hdf5matrix) for details.\n",
    "\n",
    "You can also directly use a HDF5 dataset:\n",
    "\n",
    "```python\n",
    "import h5py\n",
    "with h5py.File('input/file.hdf5', 'r') as f:\n",
    "    x_data = f['x_data']\n",
    "    model.predict(x_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is the Keras configuration file stored?\n",
    "\n",
    "The default directory where all Keras data is stored is:\n",
    "\n",
    "```bash\n",
    "$HOME/.keras/\n",
    "```\n",
    "\n",
    "Note that Windows users should replace `$HOME` with `%USERPROFILE%`.\n",
    "In case Keras cannot create the above directory (e.g. due to permission issues), `/tmp/.keras/` is used as a backup.\n",
    "\n",
    "The Keras configuration file is a JSON file stored at `$HOME/.keras/keras.json`. The default configuration file looks like this:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"image_data_format\": \"channels_last\",\n",
    "    \"epsilon\": 1e-07,\n",
    "    \"floatx\": \"float32\",\n",
    "    \"backend\": \"tensorflow\"\n",
    "}\n",
    "```\n",
    "\n",
    "It contains the following fields:\n",
    "\n",
    "- The image data format to be used as default by image processing layers and utilities (either `channels_last` or `channels_first`).\n",
    "- The `epsilon` numerical fuzz factor to be used to prevent division by zero in some operations.\n",
    "- The default float data type.\n",
    "- The default backend. See the [backend documentation](/backend).\n",
    "\n",
    "Likewise, cached dataset files, such as those downloaded with [`get_file()`](/utils/#get_file), are stored by default in `$HOME/.keras/datasets/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
